{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b3d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca92d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7685fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/mobilenet3/images/test/155.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{output_classes[0][i]}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d6d86-ea09-4082-a41e-55f2dd889ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23654c-4ec5-44b4-9f94-44adcbceeb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767e78b5-02d4-4e71-bd28-37b3c27c16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Downloads/c.png\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{output_classes[0][i]}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c937bf9-a784-43f4-b0e7-e414a9de591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819986d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Downloads/dog.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{output_classes[0][i]}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc328b1-c345-4b31-8fb0-fb6f6bec1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Downloads/reading-newspaper.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{output_classes[0][i]}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d469d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7705f341-5768-44e6-9f66-5eb3826c5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Mobilenet/images/test/20240306_154015.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{output_classes[0][i]}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ad2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239aac61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfaef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17036b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a279d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4f156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b7345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79650f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f474c750-1328-44ed-af80-720603300cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 54 57 237 252\n",
      "Title 56 25 254 152\n"
     ]
    }
   ],
   "source": [
    "#Model Testing \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune4/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Downloads/d.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "        print(label, xmin, ymin, xmax, ymax)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label}: {output_scores[0][i]:.2f}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad01a58-0c58-4eaf-b66c-e3a47e65c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "54 87 590 568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50e850-16fa-4931-9e6d-0e5b17fe8a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89386c0d-1ae8-450a-9e84-43871ce2af21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Testing \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define labels\n",
    "labels = ['newspaper', 'heading']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning3/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Downloads/dog.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label}: {output_scores[0][i]}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cac2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Testing \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define labels\n",
    "labels = ['newspaper', 'heading']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning3/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"/home/ratnakar/Mobilenet/images/test/20240306_154015.jpg\")  # Change \"test_image.jpg\" to your test image filename\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "# Preprocess image\n",
    "input_data = np.expand_dims(image_resized, axis=0)\n",
    "input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "# Draw bounding boxes\n",
    "for i in range(len(output_boxes[0])):\n",
    "    if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "        ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "        xmin = int(xmin * image.shape[1])\n",
    "        xmax = int(xmax * image.shape[1])\n",
    "        ymin = int(ymin * image.shape[0])\n",
    "        ymax = int(ymax * image.shape[0])\n",
    "        label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "cv2.putText(image, f\"{label}: {output_scores[0][i]:.2f}\", (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "# Display result\n",
    "cv2.imshow(\"Object Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb9ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d60f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a22c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0b676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb769e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4194ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define labels\n",
    "# labels = ['newspaper', 'heading']\n",
    "\n",
    "# # Load TFLite model and allocate tensors.\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/Mobilenet/finetuning3/detect.tflite\")\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get input and output tensors.\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "\n",
    "# # Path to your test image folder\n",
    "# test_image_folder = \"/home/ratnakar/Mobilenet/images/test/\"\n",
    "\n",
    "# # Loop through all images in the folder\n",
    "# for filename in os.listdir(test_image_folder):\n",
    "#     if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "#         # Load image\n",
    "#         image_path = os.path.join(test_image_folder, filename)\n",
    "#         image = cv2.imread(image_path)\n",
    "#         if image is None:\n",
    "#             print(f\"Failed to load image: {image_path}\")\n",
    "#             continue\n",
    "        \n",
    "#         image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "#         # Preprocess image\n",
    "#         input_data = np.expand_dims(image_resized, axis=0)\n",
    "#         input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "#         interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "#         # Run inference\n",
    "#         interpreter.invoke()\n",
    "\n",
    "#         # Get output\n",
    "#         output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "#         output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "#         output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "#         # Draw bounding boxes\n",
    "#         for i in range(len(output_boxes[0])):\n",
    "#             if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "#                 ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "#                 xmin = int(xmin * image.shape[1])\n",
    "#                 xmax = int(xmax * image.shape[1])\n",
    "#                 ymin = int(ymin * image.shape[0])\n",
    "#                 ymax = int(ymax * image.shape[0])\n",
    "#                 label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "#                 cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "#                 cv2.putText(image, f\"{label}: {output_scores[0][i]:.2f}\", (xmin, ymin - 5),\n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "#         # Display result\n",
    "#         cv2.imshow(\"Object Detection\", image)\n",
    "#         cv2.waitKey(0)\n",
    "\n",
    "# # Close all windows\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2919e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ba697-1c1f-493b-acb2-10b93761c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2bb41-27e5-4f29-ada6-df9f48d7dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06184b-c7cb-4245-b125-bb899b1bbb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23477ea-0b98-4e31-bf01-d29661bbf46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e39e86-4f81-4a73-8f57-a259cd17479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define labels\n",
    "# labels = ['Book', 'Title']\n",
    "\n",
    "# # Load TFLite model and allocate tensors.\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune3/detect.tflite\")\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get input and output tensors.\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "\n",
    "# # Function to process a single image\n",
    "# def process_image(image_path):\n",
    "#     # Load image\n",
    "#     image = cv2.imread(image_path)\n",
    "#     if image is None:\n",
    "#         print(f\"Could not read image {image_path}\")\n",
    "#         return\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "#     # Preprocess image\n",
    "#     input_data = np.expand_dims(image_resized, axis=0)\n",
    "#     input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "#     interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "#     # Run inference\n",
    "#     interpreter.invoke()\n",
    "\n",
    "#     # Get output\n",
    "#     output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "#     output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "#     output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "#     # Prepare to save predictions\n",
    "#     prediction_lines = []\n",
    "\n",
    "#     # Draw bounding boxes\n",
    "#     for i in range(len(output_boxes[0])):\n",
    "#         if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "#             ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "#             xmin = int(xmin * image.shape[1])\n",
    "#             xmax = int(xmax * image.shape[1])\n",
    "#             ymin = int(ymin * image.shape[0])\n",
    "#             ymax = int(ymax * image.shape[0])\n",
    "#             label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "#             print(f\"Image: {image_path}, Box: ({xmin}, {ymin}, {xmax}, {ymax}), Label: {label}, Score: {output_scores[0][i]:.2f}\")\n",
    "#             cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "#             cv2.putText(image, f\"{label}: {output_scores[0][i]:.2f}\", (xmin, ymin - 5),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "#             # Save prediction details\n",
    "#             prediction_lines.append(f\"{xmin},{ymin},{xmax},{ymax},{label},{output_scores[0][i]:.2f}\\n\")\n",
    "\n",
    "#     # Save result image\n",
    "#     output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "#     cv2.imwrite(output_image_path, image)\n",
    "\n",
    "#     # Save predictions to a text file\n",
    "#     prediction_file_path = os.path.join(prediction_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "#     with open(prediction_file_path, \"w\") as f:\n",
    "#         f.writelines(prediction_lines)\n",
    "\n",
    "# # Directory containing images\n",
    "# image_dir = \"/home/ratnakar/mobilenet3/images/train\"  # Change this to your image directory path\n",
    "# output_image_dir = \"/home/ratnakar/mobilenet3/output\"  # Change this to your output image directory path\n",
    "# prediction_dir = \"/home/ratnakar/mobilenet3/output\"  # Change this to your prediction directory path\n",
    "\n",
    "# # Create output directories if they don't exist\n",
    "# os.makedirs(output_image_dir, exist_ok=True)\n",
    "# os.makedirs(prediction_dir, exist_ok=True)\n",
    "\n",
    "# # Process each image in the directory\n",
    "# for filename in os.listdir(image_dir):\n",
    "#     if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#         image_path = os.path.join(image_dir, filename)\n",
    "#         process_image(image_path)\n",
    "\n",
    "# print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05417ed2-df65-4c86-97a9-518aa956f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune3/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    # Prepare to save predictions\n",
    "    prediction_lines = []\n",
    "\n",
    "    for book in book_predictions:\n",
    "        # Draw book bounding box\n",
    "        cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        prediction_lines.append(f\"{book['xmin']},{book['ymin']},{book['xmax']},{book['ymax']},{book['label']},{book['score']:.2f}\\n\")\n",
    "\n",
    "        # Draw best title bounding box\n",
    "        if 'best_title' in book:\n",
    "            title = book['best_title']\n",
    "            cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "            cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            prediction_lines.append(f\"{title['xmin']},{title['ymin']},{title['xmax']},{title['ymax']},{title['label']},{title['score']:.2f}\\n\")\n",
    "\n",
    "    # Save result image\n",
    "    output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_image_path, image)\n",
    "\n",
    "    # Save predictions to a text file\n",
    "    prediction_file_path = os.path.join(prediction_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "    with open(prediction_file_path, \"w\") as f:\n",
    "        f.writelines(prediction_lines)\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"/home/ratnakar/mobilenet3/images/test\"  # Change this to your image directory path\n",
    "output_image_dir = \"/home/ratnakar/mobilenet3/output/2\"  # Change this to your output image directory path\n",
    "prediction_dir = \"/home/ratnakar/mobilenet3/output/2\"  # Change this to your prediction directory path\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(prediction_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        process_image(image_path)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f87edb-205b-44cc-ba5c-44e5e6ea2929",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a423c3-70ac-4bef-b2f1-679c1cd9c298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d168fd6-a345-47ee-a80f-647edf8a07d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55004725-c2aa-4c42-a30d-2289bb200949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f446c6-6814-44d0-b195-f4e14f1220d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4d1fb-a381-4882-a846-7ef063d9bea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d5b40b-d3e0-4f1a-a391-7f39240464e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return None, None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    return book_predictions, image\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"/home/ratnakar/mobilenet3/images/test\"  # Change this to your image directory path\n",
    "output_image_dir = \"/home/ratnakar/mobilenet3/output/11\"  # Change this to your output image directory path\n",
    "prediction_dir = \"/home/ratnakar/mobilenet3/output/11\"  # Change this to your prediction directory path\n",
    "titles_dir = \"/home/ratnakar/mobilenet3/output/11\"  # Change this to your titles directory path\n",
    "cropped_titles_dir = \"/home/ratnakar/mobilenet3/output/11/cropped_titles\"  # Directory for cropped titles\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(prediction_dir, exist_ok=True)\n",
    "os.makedirs(titles_dir, exist_ok=True)\n",
    "os.makedirs(cropped_titles_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        book_predictions, image = process_image(image_path)\n",
    "        \n",
    "        if book_predictions is None:\n",
    "            continue\n",
    "        \n",
    "        # Prepare to save predictions\n",
    "        prediction_lines = []\n",
    "        title_lines = []\n",
    "\n",
    "        for book in book_predictions:\n",
    "            # Draw book bounding box\n",
    "            cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            prediction_lines.append(f\"{book['xmin']},{book['ymin']},{book['xmax']},{book['ymax']},{book['label']},{book['score']:.2f}\\n\")\n",
    "\n",
    "            # Draw best title bounding box and save the cropped title image\n",
    "            if 'best_title' in book:\n",
    "                title = book['best_title']\n",
    "                cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "                cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                prediction_lines.append(f\"{title['xmin']},{title['ymin']},{title['xmax']},{title['ymax']},{title['label']},{title['score']:.2f}\\n\")\n",
    "                title_lines.append(f\"{title['label']}:{title['score']:.2f}\\n\")\n",
    "\n",
    "                # Check if coordinates are within image bounds before cropping\n",
    "                if 0 <= title['xmin'] < title['xmax'] <= image.shape[1] and 0 <= title['ymin'] < title['ymax'] <= image.shape[0]:\n",
    "                    cropped_title = image[title['ymin']:title['ymax'], title['xmin']:title['xmax']]\n",
    "                    if cropped_title.size > 0:\n",
    "                        cropped_title_path = os.path.join(cropped_titles_dir, os.path.splitext(filename)[0] + f\"_title_{len(title_lines)}.jpg\")\n",
    "                        cv2.imwrite(cropped_title_path, cropped_title)\n",
    "                    else:\n",
    "                        print(f\"Cropped title is empty for {filename} at {title['xmin']},{title['ymin']},{title['xmax']},{title['ymax']}\")\n",
    "                else:\n",
    "                    print(f\"Invalid coordinates for {filename}: {title['xmin']},{title['ymin']},{title['xmax']},{title['ymax']}\")\n",
    "\n",
    "        # Save result image\n",
    "        output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_image_path, image)\n",
    "\n",
    "        # Save predictions to a text file\n",
    "        prediction_file_path = os.path.join(prediction_dir, os.path.splitext(os.path.basename(image_path))[0] + \".txt\")\n",
    "        with open(prediction_file_path, \"w\") as f:\n",
    "            f.writelines(prediction_lines)\n",
    "\n",
    "        # Save titles to a text file\n",
    "        title_file_path = os.path.join(titles_dir, os.path.splitext(os.path.basename(image_path))[0] + \"_titles.txt\")\n",
    "        with open(title_file_path, \"w\") as f:\n",
    "            f.writelines(title_lines)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b940c-0fdb-4fef-a6ba-1c91af4d9c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7d95c-fcce-4655-9e7a-aac5412872e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892cac9-5d5e-43a5-8fc7-750122054414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff611f-4dda-41d3-aa19-48a045859e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf70705-6e84-439a-98c6-08f75cb391ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ratnakar/anaconda3/envs/mbnet/lib/python3.7/site-packages (from pytesseract) (22.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/ratnakar/anaconda3/envs/mbnet/lib/python3.7/site-packages (from pytesseract) (9.5.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2afdeb-c934-4892-b245-9225e4ecf3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pytesseract\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return None, None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    return book_predictions, image\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"/home/ratnakar/mobilenet3/images/test_images\"  # Change this to your image directory path\n",
    "output_image_dir = \"/home/ratnakar/mobilenet3/output/1\"  # Change this to your output image directory path\n",
    "titles_dir = \"/home/ratnakar/mobilenet3/output/1\"  # Change this to your titles directory path\n",
    "cropped_titles_dir = \"/home/ratnakar/mobilenet3/output/1/cropped_titles\"  # Directory for cropped titles\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(titles_dir, exist_ok=True)\n",
    "os.makedirs(cropped_titles_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        book_predictions, image = process_image(image_path)\n",
    "        \n",
    "        if book_predictions is None:\n",
    "            continue\n",
    "        \n",
    "        title_lines = []\n",
    "        has_title = False\n",
    "\n",
    "        for book in book_predictions:\n",
    "            # Draw book bounding box\n",
    "            cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw best title bounding box and save the cropped title image\n",
    "            if 'best_title' in book:\n",
    "                title = book['best_title']\n",
    "                cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "                cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "                # Check if coordinates are within image bounds before cropping\n",
    "                if 0 <= title['xmin'] < title['xmax'] <= image.shape[1] and 0 <= title['ymin'] < title['ymax'] <= image.shape[0]:\n",
    "                    cropped_title = image[title['ymin']:title['ymax'], title['xmin']:title['xmax']]\n",
    "                    if cropped_title.size > 0:\n",
    "                        cropped_title_path = os.path.join(cropped_titles_dir, os.path.splitext(filename)[0] + f\"_title_{len(title_lines)}.jpg\")\n",
    "                        cv2.imwrite(cropped_title_path, cropped_title)\n",
    "\n",
    "                        # Perform OCR on the cropped title\n",
    "                        ocr_result = pytesseract.image_to_string(cropped_title, lang='eng').strip()\n",
    "                        title_lines.append(f\"{title['label']}: {ocr_result}\\n\")\n",
    "                        has_title = True\n",
    "\n",
    "        if has_title:\n",
    "            # Save result image\n",
    "            output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "\n",
    "            # Save titles to a text file\n",
    "            title_file_path = os.path.join(titles_dir, os.path.splitext(os.path.basename(image_path))[0] + \"_titles.txt\")\n",
    "            with open(title_file_path, \"w\") as f:\n",
    "                f.writelines(title_lines)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c692a1-e7be-40f8-9861-9780b33e67f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a8d571c-edfb-4050-9ed8-f79f138b146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books detected in 194.jpg: 0\n",
      "Number of books detected in 341.jpg: 0\n",
      "Number of books detected in 97.jpg: 0\n",
      "Number of books detected in 142.jpg: 0\n",
      "Number of books detected in 48.jpg: 0\n",
      "Number of books detected in 75.jpg: 0\n",
      "Number of books detected in 342.jpg: 1\n",
      "Number of books detected in 88.jpg: 0\n",
      "Number of books detected in 95.jpg: 0\n",
      "Number of books detected in 162.jpg: 0\n",
      "Number of books detected in 53.jpg: 0\n",
      "Number of books detected in 218.jpg: 0\n",
      "Number of books detected in 241.jpg: 1\n",
      "Number of books detected in 14.jpg: 0\n",
      "Number of books detected in 206.jpg: 0\n",
      "Number of books detected in 30.jpg: 0\n",
      "Number of books detected in 51.jpg: 0\n",
      "Number of books detected in 321.jpg: 0\n",
      "Number of books detected in 276.jpg: 0\n",
      "Number of books detected in 248.jpg: 0\n",
      "Number of books detected in 304.jpg: 0\n",
      "Number of books detected in 46.jpg: 1\n",
      "Number of books detected in 134.jpg: 1\n",
      "Number of books detected in 306.jpg: 1\n",
      "Number of books detected in 340.jpg: 1\n",
      "Number of books detected in 228.jpg: 0\n",
      "Number of books detected in 186.jpg: 1\n",
      "Number of books detected in 335.jpg: 0\n",
      "Number of books detected in 319.jpg: 0\n",
      "Number of books detected in 246.jpg: 1\n",
      "Number of books detected in 174.jpg: 0\n",
      "Number of books detected in 277.jpg: 0\n",
      "Number of books detected in 163.jpg: 1\n",
      "Number of books detected in 239.jpg: 0\n",
      "Number of books detected in 107.jpg: 1\n",
      "Number of books detected in 160.jpg: 0\n",
      "Number of books detected in 77.jpg: 0\n",
      "Number of books detected in 282.jpg: 0\n",
      "Number of books detected in 93.jpg: 0\n",
      "Number of books detected in 73.jpg: 0\n",
      "Number of books detected in 144.jpg: 0\n",
      "Number of books detected in 266.jpg: 0\n",
      "Number of books detected in 191.jpg: 0\n",
      "Number of books detected in 324.jpg: 0\n",
      "Number of books detected in 251.jpg: 0\n",
      "Number of books detected in 35.jpg: 0\n",
      "Number of books detected in 111.jpg: 0\n",
      "Number of books detected in 318.jpg: 0\n",
      "Number of books detected in 303.jpg: 0\n",
      "Number of books detected in 130.jpg: 0\n",
      "Number of books detected in 154.jpg: 0\n",
      "Number of books detected in 285.jpg: 0\n",
      "Number of books detected in 103.jpg: 0\n",
      "Number of books detected in 336.jpg: 1\n",
      "Number of books detected in 22.jpg: 0\n",
      "Number of books detected in 84.jpg: 0\n",
      "Number of books detected in 288.jpg: 1\n",
      "Number of books detected in 82.jpg: 0\n",
      "Number of books detected in 155.jpg: 1\n",
      "Number of books detected in 216.jpg: 0\n",
      "Number of books detected in 104.jpg: 1\n",
      "Number of books detected in 21.jpg: 0\n",
      "Number of books detected in 15.jpg: 0\n",
      "Number of books detected in 253.jpg: 1\n",
      "Number of books detected in 85.jpg: 0\n",
      "Number of books detected in 320.jpg: 0\n",
      "Number of books detected in 281.jpg: 0\n",
      "Number of books detected in 210.jpg: 1\n",
      "Number of books detected in 136.jpg: 0\n",
      "Number of books detected in 13.jpg: 0\n",
      "Number of books detected in 233.jpg: 1\n",
      "Number of books detected in 227.jpg: 1\n",
      "Number of books detected in 149.jpg: 0\n",
      "Number of books detected in 120.jpg: 1\n",
      "Number of books detected in 99.jpg: 1\n",
      "Number of books detected in 11.jpg: 0\n",
      "Number of books detected in 159.jpg: 1\n",
      "Number of books detected in 214.jpg: 0\n",
      "Number of books detected in 17.jpg: 1\n",
      "Number of books detected in 197.jpg: 0\n",
      "Number of books detected in 167.jpg: 1\n",
      "Number of books detected in 305.jpg: 0\n",
      "Number of books detected in 101.jpg: 1\n",
      "Number of books detected in 109.jpg: 0\n",
      "Number of books detected in 312.jpg: 1\n",
      "Number of books detected in 247.jpg: 0\n",
      "Number of books detected in 297.jpg: 1\n",
      "Number of books detected in 268.jpg: 0\n",
      "Number of books detected in 91.jpg: 1\n",
      "Number of books detected in 284.jpg: 0\n",
      "Number of books detected in 224.jpg: 0\n",
      "Number of books detected in 96.jpg: 0\n",
      "Number of books detected in 178.jpg: 0\n",
      "Number of books detected in 230.jpg: 0\n",
      "Number of books detected in 20.jpg: 0\n",
      "Number of books detected in 179.jpg: 0\n",
      "Number of books detected in 295.jpg: 1\n",
      "Number of books detected in 66.jpg: 1\n",
      "Number of books detected in 133.jpg: 0\n",
      "Number of books detected in 219.jpg: 0\n",
      "Number of books detected in 23.jpg: 1\n",
      "Number of books detected in 327.jpg: 1\n",
      "Number of books detected in 12.jpg: 0\n",
      "Number of books detected in 94.jpg: 0\n",
      "Number of books detected in 128.jpg: 1\n",
      "Number of books detected in 208.jpg: 0\n",
      "Number of books detected in 4.jpg: 0\n",
      "Number of books detected in 257.jpg: 1\n",
      "Number of books detected in 260.jpg: 0\n",
      "Number of books detected in 138.jpg: 0\n",
      "Number of books detected in 63.jpg: 0\n",
      "Number of books detected in 237.jpg: 0\n",
      "Number of books detected in 275.jpg: 0\n",
      "Number of books detected in 287.jpg: 0\n",
      "Number of books detected in 98.jpg: 0\n",
      "Number of books detected in 250.jpg: 1\n",
      "Number of books detected in 113.jpg: 0\n",
      "Number of books detected in 222.jpg: 0\n",
      "Number of books detected in 44.jpg: 1\n",
      "Number of books detected in 235.jpg: 1\n",
      "Number of books detected in 57.jpg: 0\n",
      "Number of books detected in 43.jpg: 1\n",
      "Number of books detected in 298.jpg: 1\n",
      "Number of books detected in 117.jpg: 0\n",
      "Number of books detected in 328.jpg: 0\n",
      "Number of books detected in 50.jpg: 0\n",
      "Number of books detected in 215.jpg: 0\n",
      "Number of books detected in 52.jpg: 0\n",
      "Number of books detected in 221.jpg: 0\n",
      "Number of books detected in 28.jpg: 0\n",
      "Number of books detected in 47.jpg: 0\n",
      "Number of books detected in 56.jpg: 1\n",
      "Number of books detected in 37.jpg: 1\n",
      "Number of books detected in 110.jpg: 1\n",
      "Number of books detected in 139.jpg: 0\n",
      "Number of books detected in 293.jpg: 0\n",
      "Number of books detected in 311.jpg: 0\n",
      "Number of books detected in 123.jpg: 0\n",
      "Number of books detected in 226.jpg: 1\n",
      "Number of books detected in 49.jpg: 0\n",
      "Number of books detected in 213.jpg: 0\n",
      "Number of books detected in 242.jpg: 0\n",
      "Number of books detected in 148.jpg: 0\n",
      "Number of books detected in 185.jpg: 0\n",
      "Number of books detected in 289.jpg: 1\n",
      "Number of books detected in 36.jpg: 0\n",
      "Number of books detected in 68.jpg: 0\n",
      "Number of books detected in 74.jpg: 0\n",
      "Number of books detected in 131.jpg: 0\n",
      "Number of books detected in 280.jpg: 1\n",
      "Number of books detected in ikigai.jpeg: 0\n",
      "Number of books detected in 204.jpg: 1\n",
      "Number of books detected in 33.jpg: 0\n",
      "Number of books detected in 170.jpg: 0\n",
      "Number of books detected in 180.jpg: 0\n",
      "Number of books detected in 169.jpg: 1\n",
      "Number of books detected in 301.jpg: 1\n",
      "Number of books detected in 143.jpg: 0\n",
      "Number of books detected in 72.jpg: 0\n",
      "Number of books detected in 34.jpg: 1\n",
      "Number of books detected in 70.jpg: 0\n",
      "Number of books detected in 245.jpg: 0\n",
      "Number of books detected in 240.jpg: 0\n",
      "Number of books detected in 262.jpg: 0\n",
      "Number of books detected in 171.jpg: 1\n",
      "Number of books detected in 267.jpg: 0\n",
      "Number of books detected in 338.jpg: 0\n",
      "Number of books detected in 211.jpg: 0\n",
      "Number of books detected in 165.jpg: 0\n",
      "Number of books detected in 196.jpg: 0\n",
      "Number of books detected in 337.jpg: 0\n",
      "Number of books detected in 231.jpg: 1\n",
      "Number of books detected in 141.jpg: 0\n",
      "Number of books detected in 108.jpg: 0\n",
      "Number of books detected in 265.jpg: 0\n",
      "Number of books detected in 252.jpg: 0\n",
      "Number of books detected in 151.jpg: 1\n",
      "Number of books detected in 147.jpg: 0\n",
      "Number of books detected in 279.jpg: 0\n",
      "Number of books detected in 323.jpg: 1\n",
      "Number of books detected in 168.jpg: 0\n",
      "Number of books detected in 105.jpg: 0\n",
      "Number of books detected in 16.jpg: 0\n",
      "Number of books detected in 59.jpg: 0\n",
      "Number of books detected in 83.jpg: 1\n",
      "Number of books detected in 157.jpg: 0\n",
      "Number of books detected in 310.jpg: 0\n",
      "Number of books detected in 330.jpg: 0\n",
      "Number of books detected in 331.jpg: 1\n",
      "Number of books detected in 181.jpg: 0\n",
      "Number of books detected in 249.jpg: 1\n",
      "Number of books detected in 182.jpg: 0\n",
      "Number of books detected in 24.jpg: 0\n",
      "Number of books detected in 317.jpg: 1\n",
      "Number of books detected in 54.jpg: 0\n",
      "Number of books detected in 80.jpg: 0\n",
      "Number of books detected in 286.jpg: 0\n",
      "Number of books detected in 271.jpg: 0\n",
      "Number of books detected in 156.jpg: 1\n",
      "Number of books detected in 190.jpg: 0\n",
      "Number of books detected in 58.jpg: 0\n",
      "Number of books detected in 39.jpg: 1\n",
      "Number of books detected in 258.jpg: 0\n",
      "Number of books detected in 152.jpg: 0\n",
      "Number of books detected in 25.jpg: 0\n",
      "Number of books detected in 127.jpg: 0\n",
      "Number of books detected in 274.jpg: 0\n",
      "Number of books detected in 232.jpg: 0\n",
      "Number of books detected in 200.jpg: 0\n",
      "Number of books detected in 140.jpg: 0\n",
      "Number of books detected in 90.jpg: 1\n",
      "Number of books detected in 64.jpg: 0\n",
      "Number of books detected in 299.jpg: 1\n",
      "Number of books detected in 254.jpg: 1\n",
      "Number of books detected in 38.jpg: 0\n",
      "Number of books detected in 129.jpg: 1\n",
      "Number of books detected in 166.jpg: 0\n",
      "Number of books detected in 121.jpg: 0\n",
      "Number of books detected in 42.jpg: 0\n",
      "Number of books detected in 176.jpg: 0\n",
      "Number of books detected in 217.jpg: 0\n",
      "Number of books detected in 269.jpg: 0\n",
      "Number of books detected in 238.jpg: 0\n",
      "Number of books detected in 209.jpg: 0\n",
      "Number of books detected in 203.jpg: 0\n",
      "Number of books detected in 236.jpg: 0\n",
      "Number of books detected in 223.jpg: 0\n",
      "Number of books detected in 183.jpg: 1\n",
      "Number of books detected in 102.jpg: 0\n",
      "Number of books detected in 272.jpg: 1\n",
      "Number of books detected in 198.jpg: 0\n",
      "Number of books detected in 201.jpg: 0\n",
      "Number of books detected in 189.jpg: 0\n",
      "Number of books detected in 300.jpg: 1\n",
      "Number of books detected in 339.jpg: 0\n",
      "Number of books detected in 41.jpg: 0\n",
      "Number of books detected in 158.jpg: 0\n",
      "Number of books detected in 137.jpg: 0\n",
      "Number of books detected in 150.jpg: 0\n",
      "Number of books detected in 100.jpg: 1\n",
      "Number of books detected in 199.jpg: 1\n",
      "Number of books detected in 244.jpg: 0\n",
      "Number of books detected in 135.jpg: 1\n",
      "Number of books detected in 243.jpg: 0\n",
      "Number of books detected in 87.jpg: 0\n",
      "Number of books detected in 212.jpg: 0\n",
      "Number of books detected in 125.jpg: 0\n",
      "Number of books detected in 146.jpg: 0\n",
      "Number of books detected in 184.jpg: 1\n",
      "Number of books detected in 308.jpg: 1\n",
      "Number of books detected in 255.jpg: 0\n",
      "Number of books detected in 92.jpg: 0\n",
      "Number of books detected in 302.jpg: 0\n",
      "Number of books detected in 334.jpg: 0\n",
      "Number of books detected in 313.jpg: 0\n",
      "Number of books detected in 118.jpg: 1\n",
      "Number of books detected in 172.jpg: 1\n",
      "Number of books detected in 115.jpg: 0\n",
      "Number of books detected in 61.jpg: 0\n",
      "Number of books detected in 229.jpg: 0\n",
      "Number of books detected in 31.jpg: 0\n",
      "Number of books detected in 278.jpg: 0\n",
      "Number of books detected in 294.jpg: 0\n",
      "Number of books detected in 69.jpg: 0\n",
      "Number of books detected in 132.jpg: 0\n",
      "Number of books detected in 67.jpg: 1\n",
      "Number of books detected in 333.jpg: 0\n",
      "Number of books detected in 315.jpg: 0\n",
      "Number of books detected in 76.jpg: 0\n",
      "Number of books detected in 106.jpg: 0\n",
      "Number of books detected in 188.jpg: 1\n",
      "Number of books detected in 89.jpg: 1\n",
      "Number of books detected in 332.jpg: 1\n",
      "Number of books detected in 296.jpg: 0\n",
      "Number of books detected in 256.jpg: 0\n",
      "Number of books detected in 65.jpg: 0\n",
      "Number of books detected in 307.jpg: 0\n",
      "Number of books detected in 187.jpg: 1\n",
      "Number of books detected in 205.jpg: 1\n",
      "Number of books detected in 322.jpg: 1\n",
      "Number of books detected in 18.jpg: 0\n",
      "Number of books detected in 329.jpg: 0\n",
      "Number of books detected in 40.jpg: 1\n",
      "Number of books detected in 45.jpg: 1\n",
      "Number of books detected in 314.jpg: 0\n",
      "Number of books detected in 29.jpg: 0\n",
      "Number of books detected in 207.jpg: 0\n",
      "Number of books detected in 81.jpg: 0\n",
      "Number of books detected in 126.jpg: 1\n",
      "Number of books detected in 326.jpg: 0\n",
      "Number of books detected in 1.jpg: 1\n",
      "Number of books detected in 19.jpg: 0\n",
      "Number of books detected in 202.jpg: 0\n",
      "Number of books detected in 55.jpg: 0\n",
      "Number of books detected in 220.jpg: 0\n",
      "Number of books detected in 292.jpg: 1\n",
      "Number of books detected in 112.jpg: 0\n",
      "Number of books detected in 270.jpg: 0\n",
      "Number of books detected in 177.jpg: 0\n",
      "Number of books detected in 192.jpg: 0\n",
      "Number of books detected in 325.jpg: 1\n",
      "Number of books detected in 116.jpg: 1\n",
      "Number of books detected in 79.jpg: 0\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "#Output folder 4 \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pytesseract\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune2/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return None, None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    return book_predictions, image\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"/home/ratnakar/mobilenet3/images/test_images\"  # Change this to your image directory path\n",
    "output_image_dir = \"/home/ratnakar/mobilenet3/output/1\"  # Change this to your output image directory path\n",
    "titles_dir = \"/home/ratnakar/mobilenet3/output/1\"  # Change this to your titles directory path\n",
    "cropped_titles_dir = \"/home/ratnakar/mobilenet3/output/1/cropped_titles\"  # Directory for cropped titles\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(titles_dir, exist_ok=True)\n",
    "os.makedirs(cropped_titles_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        book_predictions, image = process_image(image_path)\n",
    "        \n",
    "        if book_predictions is None:\n",
    "            continue\n",
    "        \n",
    "        title_lines = []\n",
    "        has_title = False\n",
    "\n",
    "        for book in book_predictions:\n",
    "            # Draw book bounding box\n",
    "            cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw best title bounding box and save the cropped title image\n",
    "            if 'best_title' in book:\n",
    "                title = book['best_title']\n",
    "                cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "                cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "                # Check if coordinates are within image bounds before cropping\n",
    "                if 0 <= title['xmin'] < title['xmax'] <= image.shape[1] and 0 <= title['ymin'] < title['ymax'] <= image.shape[0]:\n",
    "                    cropped_title = image[title['ymin']:title['ymax'], title['xmin']:title['xmax']]\n",
    "                    if cropped_title.size > 0:\n",
    "                        cropped_title_path = os.path.join(cropped_titles_dir, os.path.splitext(filename)[0] + f\"_title_{len(title_lines)}.jpg\")\n",
    "                        cv2.imwrite(cropped_title_path, cropped_title)\n",
    "\n",
    "                        # Perform OCR on the cropped title\n",
    "                        ocr_result = pytesseract.image_to_string(cropped_title, lang='eng').strip()\n",
    "                        title_lines.append(f\"{title['label']}: {ocr_result}\\n\")\n",
    "                        has_title = True\n",
    "\n",
    "                        # Draw OCR result on the image\n",
    "                        cv2.putText(image, f\"Title = {ocr_result}\", (title['xmin'], title['ymin'] - 25),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        if has_title:\n",
    "            # Save result image\n",
    "            output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "\n",
    "            # Save titles to a text file\n",
    "            title_file_path = os.path.join(titles_dir, os.path.splitext(os.path.basename(image_path))[0] + \"_titles.txt\")\n",
    "            with open(title_file_path, \"w\") as f:\n",
    "                f.writelines(title_lines)\n",
    "\n",
    "        # Print the number of books detected in the image\n",
    "        print(f\"Number of books detected in {filename}: {len(book_predictions)}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885ce513-8646-4d03-907b-7f1660e0e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR result for 236_title_0.jpg: S LESSONS ON WEALTH, GREED,\n",
      "AND HAPPINESS\n",
      "OCR result for 151_title_0.jpg: \n",
      "OCR result for 85_title_0.jpg: \n",
      "OCR result for 50_title_0.jpg: \n",
      "OCR result for 157_title_0.jpg: ie\n",
      "\n",
      "—\n",
      "\n",
      "ath\n",
      "\n",
      "Kr\n",
      "\n",
      "Dirit mn\n",
      "20\n",
      "poh\n",
      "PE Sie ae eels\n",
      "OCR result for 126_title_0.jpg: \n",
      "OCR result for 100_title_0.jpg: \n",
      "OCR result for 301_title_0.jpg: TAY\n",
      "HUSBANDS |\n",
      "OCR result for 305_title_0.jpg: \n",
      "OCR result for 279_title_0.jpg: MAGDALEN LIBR\n",
      "INS + OUTS\n",
      "2024\n",
      "OCR result for 11_title_0.jpg: \n",
      "OCR result for 191_title_0.jpg: : O78.\n",
      "\n",
      "ibooksjijthought)\n",
      "‘would!be¥s}\n",
      "\n",
      "TtO\n",
      "OCR result for 121_title_0.jpg: \n",
      "OCR result for 242_title_0.jpg: ci\n",
      "\n",
      "Atomic\n",
      "Habit S\n",
      "vos &\n",
      "\n",
      "James Clear\n",
      "OCR result for 66_title_0.jpg: \n",
      "OCR result for 212_title_0.jpg: \n",
      "OCR result for 339_title_0.jpg: MAGDALEN LIBRA\n",
      "INS + OUTS\n",
      "2024\n",
      "OCR result for 296_title_0.jpg: Xf eye\n",
      "OCR result for 253_title_0.jpg: P INTERNATIONAL BESTSELLER\n",
      "\n",
      "Tiny Changes,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "An Easy & Proven Way ity\n",
      "\n",
      "uh\n",
      "reysT ts\n",
      "OCR result for 248_title_0.jpg: \n",
      "OCR result for 208_title_0.jpg: @ Book haul\n",
      "library editic\n",
      "OCR result for 149_title_0.jpg: LES Eee\n",
      "\n",
      "mes, board games, puzzles, books and\n",
      "boks\n",
      "\n",
      ", get 1\n",
      "\n",
      "TY om Oe\n",
      "OCR result for 80_title_0.jpg: BOOT: AL SPACE ODYSSEY\n",
      "\n",
      "EN\n",
      "OCR result for 31_title_0.jpg: AIRSICK\n",
      "OCR result for 91_title_0.jpg: ais rat ogneé\n",
      "OCR result for 155_title_0.jpg: ro!\n",
      "OCR result for 274_title_0.jpg: \n",
      "OCR result for 123_title_0.jpg: \n",
      "OCR result for 308_title_0.jpg: WHAT IT MEA\n",
      "TO BE MIXEDR\n",
      "\n",
      " \n",
      "\n",
      "REMI ADEKO\n",
      "\n",
      "‘Ground-\n",
      "\n",
      "COSMO)\n",
      "OCR result for 146_title_0.jpg: \n",
      "OCR result for 322_title_0.jpg: \n",
      "OCR result for 92_title_1.jpg: \n",
      "OCR result for 117_title_0.jpg: \n",
      "OCR result for 330_title_0.jpg: \n",
      "OCR result for 96_title_0.jpg: \n",
      "OCR result for 111_title_0.jpg: \n",
      "OCR result for 249_title_0.jpg: \n",
      "OCR result for 21_title_0.jpg: \n",
      "OCR result for 188_title_0.jpg: MAGDALEN LIBRA\n",
      "\n",
      "INS + OUTS\n",
      "OCR result for 268_title_0.jpg: \n",
      "OCR result for 133_title_0.jpg: \n",
      "OCR result for 232_title_0.jpg: LESSONS ON WEALTH, GRE\n",
      "AND HAPPINESS\n",
      "\n",
      "RGAN HOUSEL\n",
      "OCR result for 255_title_0.jpg: \n",
      "OCR result for 102_title_0.jpg: { adc\n",
      "in Mine\n",
      "\n",
      "STRANGE STORIES\n",
      "\n",
      "ROBERT\n",
      ". AICKMANY\n",
      "OCR result for 204_title_0.jpg: \n",
      "OCR result for 144_title_0.jpg: \n",
      "OCR result for 102_title_1.jpg: \n",
      "OCR result for 97_title_0.jpg: TIME, A\n",
      "FALCONER\n",
      "OCR result for 158_title_0.jpg: \n",
      "OCR result for 113_title_0.jpg: \n",
      "OCR result for 102_title_2.jpg: \n",
      "OCR result for 134_title_0.jpg: \n",
      "OCR result for 118_title_0.jpg: \n",
      "OCR result for 336_title_0.jpg: CRU Ld\n",
      "aaa eS UCAS\n",
      "OCR result for 135_title_0.jpg: \n",
      "OCR result for 92_title_0.jpg: PLUSSISCHC\n",
      "Monastere E\n",
      "OCR result for 190_title_0.jpg: \n",
      "OCR result for 125_title_0.jpg: \n",
      "OCR result for 256_title_0.jpg: (cr |\n",
      "dus\n",
      "OCR result for 315_title_0.jpg: - ae =\n",
      "OCR result for 297_title_0.jpg: LYNN PAINTER\n",
      "OCR result for 325_title_0.jpg: \n",
      "OCR result for 338_title_0.jpg: \n",
      "OCR result for 241_title_0.jpg: Aveanqgeee\n",
      "he Weaiille\n",
      "\n",
      "mic\n",
      "OCR result for 250_title_0.jpg: \n",
      "OCR result for 108_title_0.jpg: Pou. tee\n",
      "SOBRE ALE]\n",
      "alo\n",
      "\n",
      " \n",
      "\n",
      "ANTICORRUp\n",
      "es ey eo 7\n",
      "OCR result for 214_title_0.jpg: @ Book haul\n",
      "\n",
      " \n",
      "\n",
      "library editic\n",
      "OCR result for 240_title_0.jpg: \n",
      "OCR result for 226_title_0.jpg: \n",
      "OCR result for 332_title_0.jpg: ANXIOU:\n",
      "FECTIONIS\n",
      "\n",
      "HOW TO MANAGE\n",
      "OCR result for 139_title_0.jpg: \n",
      "OCR result for 197_title_0.jpg: \n",
      "OCR result for 243_title_0.jpg: \n",
      "OCR result for 311_title_0.jpg: bookswthanwen\n",
      "acunllyS\n",
      "OCR result for 148_title_0.jpg: \n",
      "OCR result for 329_title_0.jpg: \n",
      "OCR result for 314_title_0.jpg: \n",
      "OCR processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Directory containing cropped title images\n",
    "cropped_titles_dir = \"/home/ratnakar/mobilenet3/output/5/cropped_titles\"  # Change this to your cropped titles directory path\n",
    "output_dir = \"/home/ratnakar/mobilenet3/output/5/ocr_titles\"  # Directory for output images with OCR\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the cropped titles directory\n",
    "for filename in os.listdir(cropped_titles_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(cropped_titles_dir, filename)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Could not read image {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Perform OCR on the image\n",
    "        ocr_result = pytesseract.image_to_string(image, lang='eng').strip()\n",
    "        \n",
    "        # Draw OCR result on the image\n",
    "        ocr_output_image = image.copy()\n",
    "        cv2.putText(ocr_output_image, f\"OCR: {ocr_result}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        # Save the image with OCR result\n",
    "        output_image_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_image_path, ocr_output_image)\n",
    "        \n",
    "        # Save the OCR result to a text file\n",
    "        output_text_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \"_ocr.txt\")\n",
    "        with open(output_text_path, \"w\") as f:\n",
    "            f.write(ocr_result)\n",
    "        \n",
    "        # Print the OCR result\n",
    "        print(f\"OCR result for {filename}: {ocr_result}\")\n",
    "\n",
    "print(\"OCR processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f7674b-99bf-4b9d-abe2-70a64cc2b308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books detected in ikigai.jpeg: 2\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pytesseract\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune3/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return None, None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    return book_predictions, image\n",
    "\n",
    "# Define paths\n",
    "image_path = \"/home/ratnakar/mobilenet3/images/test_images/ikigai.jpeg\"  # Change this to your image file path\n",
    "output_image_dir = \"/home/ratnakar/mobilenet3/output/6\"  # Change this to your output image directory path\n",
    "titles_dir = \"/home/ratnakar/mobilenet3/output/6\"  # Change this to your titles directory path\n",
    "cropped_titles_dir = \"/home/ratnakar/mobilenet3/output/6/cropped_titles\"  # Directory for cropped titles\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(titles_dir, exist_ok=True)\n",
    "os.makedirs(cropped_titles_dir, exist_ok=True)\n",
    "\n",
    "# Process the image\n",
    "book_predictions, image = process_image(image_path)\n",
    "\n",
    "if book_predictions:\n",
    "    title_lines = []\n",
    "    has_title = False\n",
    "\n",
    "    for book in book_predictions:\n",
    "        # Draw book bounding box\n",
    "        cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw best title bounding box and save the cropped title image\n",
    "        if 'best_title' in book:\n",
    "            title = book['best_title']\n",
    "            cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "            cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "            # Check if coordinates are within image bounds before cropping\n",
    "            if 0 <= title['xmin'] < title['xmax'] <= image.shape[1] and 0 <= title['ymin'] < title['ymax'] <= image.shape[0]:\n",
    "                cropped_title = image[title['ymin']:title['ymax'], title['xmin']:title['xmax']]\n",
    "                if cropped_title.size > 0:\n",
    "                    cropped_title_path = os.path.join(cropped_titles_dir, os.path.splitext(os.path.basename(image_path))[0] + \"_title.jpg\")\n",
    "                    cv2.imwrite(cropped_title_path, cropped_title)\n",
    "\n",
    "                    # Perform OCR on the cropped title\n",
    "                    ocr_result = pytesseract.image_to_string(cropped_title, lang='eng').strip()\n",
    "                    title_lines.append(f\"{title['label']}: {ocr_result}\\n\")\n",
    "                    has_title = True\n",
    "\n",
    "                    # Draw OCR result on the image\n",
    "                    cv2.putText(image, f\"Title = {ocr_result}\", (title['xmin'], title['ymin'] - 25),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    if has_title:\n",
    "        # Save result image\n",
    "        output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_image_path, image)\n",
    "\n",
    "        # Save titles to a text file\n",
    "        title_file_path = os.path.join(titles_dir, os.path.splitext(os.path.basename(image_path))[0] + \"_titles.txt\")\n",
    "        with open(title_file_path, \"w\") as f:\n",
    "            f.writelines(title_lines)\n",
    "\n",
    "    # Print the number of books detected in the image\n",
    "    print(f\"Number of books detected in {os.path.basename(image_path)}: {len(book_predictions)}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f971b3-1ba7-41c0-916e-111eee1ab243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books detected in ikigai.jpeg: 2\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pytesseract\n",
    "\n",
    "# Define labels\n",
    "labels = ['Book', 'Title']\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"/home/ratnakar/mobilenet3/finetune3/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not read image {image_path}\")\n",
    "        return None, None\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (300, 300))  # Resize image to match model input size\n",
    "\n",
    "    # Preprocess image\n",
    "    input_data = np.expand_dims(image_resized, axis=0)\n",
    "    input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get output\n",
    "    output_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    output_scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "\n",
    "    # Initialize variables to keep track of the highest scoring title for each book\n",
    "    book_predictions = []\n",
    "\n",
    "    # Filter out boxes that are too large or have low confidence\n",
    "    for i in range(len(output_boxes[0])):\n",
    "        if output_scores[0][i] > 0.5:  # Confidence threshold\n",
    "            ymin, xmin, ymax, xmax = output_boxes[0][i]\n",
    "            box_width = xmax - xmin\n",
    "            box_height = ymax - ymin\n",
    "            if box_width < 0.9 and box_height < 0.9:  # Filter out boxes that are too large\n",
    "                xmin = int(xmin * image.shape[1])\n",
    "                xmax = int(xmax * image.shape[1])\n",
    "                ymin = int(ymin * image.shape[0])\n",
    "                ymax = int(ymax * image.shape[0])\n",
    "                label = labels[int(output_classes[0][i])]  # Get label corresponding to predicted class index\n",
    "\n",
    "                if label == 'Book':\n",
    "                    book_predictions.append({\n",
    "                        'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                        'label': label, 'score': output_scores[0][i], 'titles': []\n",
    "                    })\n",
    "                elif label == 'Title':\n",
    "                    if book_predictions:\n",
    "                        book_predictions[-1]['titles'].append({\n",
    "                            'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax,\n",
    "                            'label': label, 'score': output_scores[0][i]\n",
    "                        })\n",
    "\n",
    "    # Keep the title with the highest score for each book\n",
    "    for book in book_predictions:\n",
    "        if book['titles']:\n",
    "            best_title = max(book['titles'], key=lambda x: x['score'])\n",
    "            book['best_title'] = best_title\n",
    "\n",
    "    return book_predictions, image\n",
    "\n",
    "# Define paths\n",
    "image_path = \"/home/ratnakar/mobilenet3/images/test_images/ikigai.jpeg\"  # Change this to your image file path\n",
    "\n",
    "# Process the image\n",
    "book_predictions, image = process_image(image_path)\n",
    "\n",
    "if book_predictions:\n",
    "    title_lines = []\n",
    "    has_title = False\n",
    "\n",
    "    for book in book_predictions:\n",
    "        # Draw book bounding box\n",
    "        cv2.rectangle(image, (book['xmin'], book['ymin']), (book['xmax'], book['ymax']), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{book['label']}: {book['score']:.2f}\", (book['xmin'], book['ymin'] - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw best title bounding box and save the cropped title image\n",
    "        if 'best_title' in book:\n",
    "            title = book['best_title']\n",
    "            cv2.rectangle(image, (title['xmin'], title['ymin']), (title['xmax'], title['ymax']), (255, 0, 0), 2)\n",
    "            cv2.putText(image, f\"{title['label']}: {title['score']:.2f}\", (title['xmin'], title['ymin'] - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "            # Check if coordinates are within image bounds before cropping\n",
    "            if 0 <= title['xmin'] < title['xmax'] <= image.shape[1] and 0 <= title['ymin'] < title['ymax'] <= image.shape[0]:\n",
    "                cropped_title = image[title['ymin']:title['ymax'], title['xmin']:title['xmax']]\n",
    "                if cropped_title.size > 0:\n",
    "                    # Perform OCR on the cropped title\n",
    "                    ocr_result = pytesseract.image_to_string(cropped_title, lang='eng').strip()\n",
    "                    title_lines.append(f\"{title['label']}: {ocr_result}\\n\")\n",
    "                    has_title = True\n",
    "\n",
    "                    # Draw OCR result on the image\n",
    "                    cv2.putText(image, f\"Title = {ocr_result}\", (title['xmin'], title['ymin'] - 25),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the result image\n",
    "    cv2.imshow('Processed Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print the number of books detected in the image\n",
    "    print(f\"Number of books detected in {os.path.basename(image_path)}: {len(book_predictions)}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c002f-bdee-4b11-ba3e-a574ea46eb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
